{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7ae8662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 16:32:35 WARN Utils: Your hostname, gosroth resolves to a loopback address: 127.0.1.1; using 192.168.1.242 instead (on interface enp4s0)\n",
      "22/04/07 16:32:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/david/.local/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/07 16:32:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, DateType, TimestampType\n",
    "from pyspark.sql.functions import *\n",
    "import csv\n",
    "\n",
    "sc = SparkContext(\"local\", \"etl\")\n",
    "spark = SparkSession.builder.config(\"spark.executor.memory\", \"12g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face83e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic preprocessing to fix some odd issues in the CSV file.\n",
    "# Mainly there are quote marks inside quoted text in the long marketing description fields.\n",
    "# For some reason it looks like instead of an escape character like \\ they used two quotation marks in a row\n",
    "# to indicate a quote.  But pyspark does not process that well when reading in a dataframe and since these\n",
    "# marketing description fields are multiline it can confuse things if the quotes aren't escaped properly.\n",
    "#\n",
    "# Doing this in standard python code would be very slow given the file size.  I probably could use spark if I\n",
    "# read the file in as an RDD first, applied the regex there, then converted it to a dataframe.  But in the interest\n",
    "# of keeping things simple I just used sed which actually can process large files like this pretty efficiently\n",
    "#\n",
    "# Note I did this on a copy because it modifies the file in place and I wanted to keep the original in case I\n",
    "# needed it for something but of course this would work on the original too.\n",
    "#\n",
    "# \n",
    "\n",
    "!sed -i 's/\"\"//g' renthub_data_copy.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc1e7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- beds: integer (nullable = true)\n",
      " |-- baths: double (nullable = true)\n",
      " |-- sqft: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- posted_at: timestamp (nullable = true)\n",
      " |-- marketing_desc: string (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- scraped_at: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Just read in the file as a dataframe here with a fixed schema\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"state\", StringType()),\n",
    "    StructField(\"city\", StringType()),\n",
    "    StructField(\"zip\", StringType()),\n",
    "    StructField(\"address\", StringType()),\n",
    "    StructField(\"property_type\", StringType()),\n",
    "    StructField(\"beds\", IntegerType()),\n",
    "    StructField(\"baths\", DoubleType()),\n",
    "    StructField(\"sqft\", IntegerType()),\n",
    "    StructField(\"price\", IntegerType()),\n",
    "    StructField(\"lat\", DoubleType()),\n",
    "    StructField(\"long\", DoubleType()),\n",
    "    StructField(\"posted_at\", TimestampType()),\n",
    "    StructField(\"marketing_desc\", StringType()),\n",
    "    StructField(\"company\", StringType()),\n",
    "    StructField(\"scraped_at\", TimestampType())\n",
    "])\n",
    "\n",
    "df = spark.read.schema(schema).option(\"multiline\", \"true\").option(\"escape\", \"\\\\\").csv(\"renthub_data_copy.csv\", header=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87999b7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+---------+-----+-------------------+-------------+----+-----+----+-----+----------+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|       id|state|     city|  zip|            address|property_type|beds|baths|sqft|price|       lat|       long|           posted_at|      marketing_desc|             company|          scraped_at|\n",
      "+---------+-----+---------+-----+-------------------+-------------+----+-----+----+-----+----------+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|316859180|   IL|  Chicago|60610|      805 N Laselle| Multi-family|   0|  1.0| 584| 1681|  41.89687| -87.632466|2019-06-07 19:04:...|This rental prope...|                null|2019-06-07 19:04:...|\n",
      "|316859617|   MA|   Boston|02134|1 President Terrace| Multi-family|   1|  1.0|null| 1825|42.3534811|-71.1290716|2019-06-07 19:05:...|1 Bed 1 Bath Spli...|Boardwalk Properties|2019-06-07 19:05:...|\n",
      "|316859618|   MA|   Boston|02134|1 President Terrace| Multi-family|   1|  1.0|null| 1825|42.3534811|-71.1290716|2019-06-07 19:05:...|1 Bed 1 Bath Spli...|Boardwalk Properties|2019-06-07 19:05:...|\n",
      "|316859619|   MA|   Boston|02134|1 President Terrace| Multi-family|   1|  1.0|null| 1825|42.3534811|-71.1290716|2019-06-07 19:05:...|1 Bed 1 Bath Spli...|Boardwalk Properties|2019-06-07 19:05:...|\n",
      "|316859620|   MA|   Boston|02134|1 President Terrace| Multi-family|   1|  1.0|null| 1825|42.3534811|-71.1290716|2019-06-07 19:05:...|1 Bed 1 Bath Spli...|Boardwalk Properties|2019-06-07 19:05:...|\n",
      "|316859621|   MA|   Boston|02134|1 President Terrace| Multi-family|   1|  1.0|null| 1825|42.3534811|-71.1290716|2019-06-07 19:05:...|1 Bed 1 Bath Spli...|Boardwalk Properties|2019-06-07 19:05:...|\n",
      "|316859622|   MA|   Boston|02134|1 President Terrace| Multi-family|   3|  2.0|null| 2600|42.3534811|-71.1290716|2019-06-07 19:05:...|1 Bed 1 Bath Spli...|Boardwalk Properties|2019-06-07 19:05:...|\n",
      "|316859623|   MA|   Boston|02134|1 President Terrace| Multi-family|   3|  2.0|null| 2600|42.3534811|-71.1290716|2019-06-07 19:05:...|1 Bed 1 Bath Spli...|Boardwalk Properties|2019-06-07 19:05:...|\n",
      "|316859624|   MA|Cambridge|02139|    33 Inman Street| Multi-family|   1|  1.0| 700| 2200|42.3682499|-71.1042948|2019-06-07 19:05:...|Beautiful 1BD/1BA...|                null|2019-06-07 19:05:...|\n",
      "|316859627|   MA|   Boston|02134|1 President Terrace| Multi-family|   1|  1.0|null| 1825|42.3534811|-71.1290716|2019-06-07 19:05:...|1 Bed 1 Bath Spli...|Boardwalk Properties|2019-06-07 19:05:...|\n",
      "+---------+-----+---------+-----+-------------------+-------------+----+-----+----+-----+----------+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3900597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will filter on the state field to eliminate some stray rows with odd values.  I do allow for both abbreviations\n",
    "# and full names though.\n",
    "# Note that I am only using US states here but it would be easy enough to add Canadian provences, etc. to the list\n",
    "\n",
    "valid_names = []\n",
    "state_mapping = {}\n",
    "with open('name-abbr.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    for row in reader:\n",
    "        valid_names.append(row[0])\n",
    "        valid_names.append(row[1])\n",
    "        state_mapping[row[0]] = row[1]\n",
    "        \n",
    "df = df.filter(df.state.isin(valid_names) & df.zip.isNotNull() & df.price.isNotNull())\n",
    "#df.select('state').distinct().show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8df1f460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make UDFs to transform state and zipcode fields\n",
    "# Any states with a name instead of abbreviation will be converted to the abbreviation\n",
    "# Any zip codes with a dash will be converted to the simple 5 digit zip\n",
    "\n",
    "# Note: I didn't end up using the state UDF because I realized I didn't really need it\n",
    "\n",
    "def convert_state(state):\n",
    "    if state in state_mapping:\n",
    "        return state_mapping[state]\n",
    "    else:\n",
    "        return state\n",
    "    \n",
    "convert_state_udf = udf(lambda z: convert_state(z), StringType())\n",
    "#df.select(convert_state_udf(col(\"state\")).alias(\"state\")).distinct().show(100)\n",
    "\n",
    "def convert_zip(zip):\n",
    "    if zip and \"-\" in zip:\n",
    "        return zip.split(\"-\")[0]\n",
    "    else:\n",
    "        return zip\n",
    "\n",
    "convert_zip_udf = udf(lambda z: convert_zip(z), StringType())\n",
    "#df.select(convert_zip_udf(col(\"zip\")).alias(\"zip\")).distinct().show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a281c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split listings by month and eliminate duplicates\n",
    "# Zip, and address fields are used to distinguish actually distinct listings.  \n",
    "\n",
    "reduced_df = df.select(convert_zip_udf(col(\"zip\")).alias(\"zip\"),\n",
    "                       'address', 'price', year(\"posted_at\").alias(\"year\"),\n",
    "                       month(\"posted_at\").alias(\"month\"))\n",
    "reduced_df = reduced_df.groupBy('zip', 'address', 'year', 'month').avg(\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f74bd1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle cases where there is insufficient data\n",
    "# Sufficient wasn't really defined so I'll go with at least 10 listings per month per zip\n",
    "# First I make a dataframe with counts for each zip for every month, then I'll join that with the main df\n",
    "\n",
    "min_listings = 10\n",
    "\n",
    "count_df = reduced_df.groupBy('zip', 'year', 'month').count()\n",
    "count_df = count_df.filter(col(\"count\") >= min_listings)\n",
    "# count_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30d04f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now join as a filter for zip codes and months with insufficient entries\n",
    "reduced_df = reduced_df.join(count_df, ['zip', 'year', 'month'])\n",
    "#reduced_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85299fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 16:37:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write output\n",
    "\n",
    "# Output schema is just:\n",
    "# zip\n",
    "# year\n",
    "# month\n",
    "# price\n",
    "#\n",
    "# More information certainly could have been included (for instance sqft, beds, and baths to know more about the\n",
    "# particular property) but that's not used in the current analysis and the stated objective was to keep storage\n",
    "# requirements low\n",
    "\n",
    "reduced_df.select(\"zip\", \"year\", \"month\", col(\"avg(price)\").alias(\"price\")).write.parquet(\"output_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed8c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
